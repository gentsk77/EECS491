{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EECS 491 Assignment 3\n",
    "\n",
    "  Yue Shu  \n",
    "  Spring 2019  \n",
    "  Prof. Lewicki  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1. Multivariate Gaussians\n",
    "\n",
    "## 1.1 Consider the 2D normal distribution \n",
    "\n",
    "**$$ p(x,y) \\sim \\mathcal{N}(\\mathbf{\\mu}, \\mathbf{\\Sigma}) $$**\n",
    "\n",
    "**Define three separate 2D covariance matrices $\\mathbf{\\Sigma}$ for each of the following cases: $x$ and $y$ are uncorrelated; $x$ and $y$ are correlated; and $x$ and $y$ are anti-correlated.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance matrix $\\Sigma$ could be defined as below in general:\n",
    "\n",
    "$$  \n",
    "\\Sigma =  \\begin{bmatrix} \\Sigma_{x x} \\space \\Sigma_{x y} \\\\ \\Sigma_{y x} \\space \\Sigma_{y y} \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $x$ and $y$ are uncorrelated\n",
    "\n",
    "Since $x$ and $y$ are uncorrelated, we may conclude that $\\Sigma_{x y} = \\Sigma_{y x} = 0$, and thus the convariance matrix should be defined as:\n",
    "\n",
    "$$  \n",
    "\\Sigma =  \\begin{bmatrix} var(x) \\space \\space \\space \\space 0 \\\\ 0 \\space \\space \\space \\space var(y) \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### $x$ and $y$ are correlated\n",
    "\n",
    "Since $x$ and $y$ are positively correlated, we may conclude that $\\Sigma_{x y} > 0$, $\\Sigma_{y x} > 0$, and thus the convariance matrix should be defined as:\n",
    "\n",
    "$$  \n",
    "\\Sigma =  \\begin{bmatrix} var(x) \\space \\space \\space \\space \\Sigma_{x y} \\\\ \\Sigma_{y x} \\space \\space \\space \\space var(y) \\end{bmatrix}, \\Sigma_{x y} > 0, \\Sigma_{y x} > 0\n",
    "$$\n",
    "\n",
    "### $x$ and $y$ are anti-correlated\n",
    "\n",
    "Since $x$ and $y$ are negatively correlated, we may conclude that $\\Sigma_{x y} < 0$, $\\Sigma_{y x} < 0$, and thus the convariance matrix should be defined as:\n",
    "\n",
    "$$  \n",
    "\\Sigma =  \\begin{bmatrix} var(x) \\space \\space \\space \\space \\Sigma_{x y} \\\\ \\Sigma_{y x} \\space \\space \\space \\space var(y) \\end{bmatrix}, \\Sigma_{x y} < 0, \\Sigma_{y x} < 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Compute the principal axes for each of these distributions, i.e. the eigenvectors of the covariance matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression of eigenvector and eigenvalue is \n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}\n",
    "$$\n",
    "\n",
    "### $x$ and $y$ are uncorrelated\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} var(x) \\space \\space \\space \\space 0 \\\\ 0 \\space \\space \\space \\space var(y) \\end{bmatrix} \\mathbf{v} = \\lambda \\mathbf{v}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} var(x) \\space \\space \\space \\space 0 \\\\ 0 \\space \\space \\space \\space var(y) \\end{bmatrix} \\mathbf{v} = \\lambda \\mathbf{v}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} var(x) \\space \\space \\space \\space 0 \\\\ 0 \\space \\space \\space \\space var(y) \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} \\lambda v_1 \\\\ \\lambda v_2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} var(x) v_1 \\\\ var(y) v_2 \\end{bmatrix}  = \\begin{bmatrix} \\lambda v_1 \\\\ \\lambda v_2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Therefore, if $var(x) \\neq var(y)$, we may conclude that the eigenvector does not exist. \n",
    "\n",
    "However, if $var(x) = var(y)$, which means $\\lambda = var(x) = var(y)$, then any vector would suffice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $x$ and $y$ are correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps for positively correlated and negatively correlated $x$ and $y$ should be the same, since the only difference is the sign of the covariance, which is only represented in the actual calculations. \n",
    "\n",
    "$$  \n",
    "\\begin{bmatrix} var(x) \\space \\space \\space \\space \\Sigma_{x y} \\\\ \\Sigma_{y x} \\space \\space \\space \\space var(y) \\end{bmatrix}  \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} \\lambda v_1 \\\\ \\lambda v_2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} var(x) v_1 + \\Sigma_{xy} v_2 \\\\ \\Sigma_{yx} v_1 + var(y) v_2 \\end{bmatrix}  = \\begin{bmatrix} \\lambda v_1 \\\\ \\lambda v_2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_{xy} v_2 = (\\lambda - var(x)v_1)v_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_{yx}v_1 = (\\lambda - var(y) v_2)v_2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_{xy} v_2 = \\lambda v_1 - var(x)v_1^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_2 = \\frac{\\lambda v_1 - var(x)v_1^2}{\\Sigma_{xy}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_1 = \\frac{\\lambda v_2 - var(y)v_2^2}{\\Sigma_{yx}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_1 = \\frac{\\lambda \\frac{\\lambda v_1 - var(x)v_1^2}{\\Sigma_{xy}} - var(y)( \\frac{\\lambda v_1 - var(x)v_1^2}{\\Sigma_{xy}})^2}{\\Sigma_{yx}}\n",
    "$$\n",
    "\n",
    "After $v_1$ is solved, we just go back and solve for $v_2$. I will put a stop here since the process should be pretty trivial and in reality once we've gain the actual covariance matrix it should be pretty straight forward. \n",
    "\n",
    "The steps of solving for the eigenvalue $\\lambda$ should also be quite simple:\n",
    "\n",
    "$$\n",
    "\\Sigma - \\lambda I = \\begin{bmatrix} var(x) \\space \\space \\space \\space \\Sigma_{x y} \\\\ \\Sigma_{y x} \\space \\space \\space \\space var(y) \\end{bmatrix} - \\begin{bmatrix} \\lambda \\space \\space 0 \\\\ 0 \\space \\space \\lambda  \\end{bmatrix} = \\begin{bmatrix} var(x) - \\lambda \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\Sigma_{xy} \\\\ \\Sigma_{yx} \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space var(y) - \\lambda  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Then we use the determinant to solve for the eigenvalue:\n",
    "\n",
    "$$\n",
    "var(x)var(y) + \\lambda ^2 - \\lambda var(x) - \\lambda var(y) = \\Sigma_{xy} \\Sigma_{yx}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\lambda ^2 - ( var(x) + var(y)) \\lambda - \\Sigma_{xy} \\Sigma_{yx} + var(x)var(y) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda_1 = \\frac{var(x) + var(y) + \\sqrt{( var(x) - var(y))^2 - 4 \\Sigma_{xy} \\Sigma_{yx}}}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda_2 = \\frac{var(x) + var(y) - \\sqrt{( var(x) - var(y))^2 - 4\\Sigma_{xy} \\Sigma_{yx}}}{2}\n",
    "$$\n",
    "\n",
    "All we should do next is to plug in the two eigenvalues, check which one is valid, and then solve for the corresponding eigenvector. Once again, I will pause my solution here since solving for the eigenvectors without any actual value is quite trivial, and as long as the steps I listed above are strictly followed, the calculation should be quite simple. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
