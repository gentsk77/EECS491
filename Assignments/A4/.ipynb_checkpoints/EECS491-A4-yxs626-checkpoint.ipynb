{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EECS 491 Assignment 3\n",
    "\n",
    "  Yue Shu  \n",
    "  Spring 2019  \n",
    "  Prof. Lewicki  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1. Multivariate Gaussians\n",
    "\n",
    "## 1.1 Consider the 2D normal distribution \n",
    "\n",
    "**$$ p(x,y) \\sim \\mathcal{N}(\\mathbf{\\mu}, \\mathbf{\\Sigma}) $$**\n",
    "\n",
    "**Define three separate 2D covariance matrices $\\mathbf{\\Sigma}$ for each of the following cases: $x$ and $y$ are uncorrelated; $x$ and $y$ are correlated; and $x$ and $y$ are anti-correlated.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance matrix $\\Sigma$ could be defined as below in general:\n",
    "\n",
    "$$  \n",
    "\\Sigma =  \\begin{bmatrix} \\Sigma_{x x} & \\Sigma_{x y} \\\\ \\Sigma_{y x} & \\Sigma_{y y} \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $x$ and $y$ are uncorrelated\n",
    "\n",
    "Since $x$ and $y$ are uncorrelated, we may conclude that $\\Sigma_{x y} = \\Sigma_{y x} = 0$, and thus the convariance matrix should be defined as:\n",
    "\n",
    "$$  \n",
    "\\Sigma =  \\begin{bmatrix} var(x) & 0 \\\\ 0 & var(y) \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### $x$ and $y$ are correlated\n",
    "\n",
    "Since $x$ and $y$ are positively correlated, we may conclude that $\\Sigma_{x y} > 0$, $\\Sigma_{y x} > 0$, and thus the convariance matrix should be defined as:\n",
    "\n",
    "$$  \n",
    "\\Sigma =  \\begin{bmatrix} var(x) & \\Sigma_{x y} \\\\ \\Sigma_{y x} & var(y) \\end{bmatrix}, \\Sigma_{x y} > 0, \\Sigma_{y x} > 0\n",
    "$$\n",
    "\n",
    "### $x$ and $y$ are anti-correlated\n",
    "\n",
    "Since $x$ and $y$ are negatively correlated, we may conclude that $\\Sigma_{x y} < 0$, $\\Sigma_{y x} < 0$, and thus the convariance matrix should be defined as:\n",
    "\n",
    "$$  \n",
    "\\Sigma =  \\begin{bmatrix} var(x) & \\Sigma_{x y} \\\\ \\Sigma_{y x} & var(y) \\end{bmatrix}, \\Sigma_{x y} < 0, \\Sigma_{y x} < 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Compute the principal axes for each of these distributions, i.e. the eigenvectors of the covariance matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression of eigenvector and eigenvalue is \n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}\n",
    "$$\n",
    "\n",
    "### $x$ and $y$ are uncorrelated\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} var(x) & 0 \\\\ 0 & var(y) \\end{bmatrix} \\mathbf{v} = \\lambda \\mathbf{v}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} var(x) & 0 \\\\ 0 & var(y) \\end{bmatrix} \\mathbf{v} = \\lambda \\mathbf{v}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} var(x) & 0 \\\\ 0 & var(y) \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} \\lambda v_1 \\\\ \\lambda v_2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} var(x) v_1 \\\\ var(y) v_2 \\end{bmatrix}  = \\begin{bmatrix} \\lambda v_1 \\\\ \\lambda v_2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Therefore, if $var(x) \\neq var(y)$, we may conclude that the eigenvector does not exist. \n",
    "\n",
    "However, if $var(x) = var(y)$, which means $\\lambda = var(x) = var(y)$, then any vector would suffice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $x$ and $y$ are correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps for positively correlated and negatively correlated $x$ and $y$ should be the same, since the only difference is the sign of the covariance, which is only represented in the actual calculations. \n",
    "\n",
    "$$  \n",
    "\\begin{bmatrix} var(x) & \\Sigma_{x y} \\\\ \\Sigma_{y x} & var(y) \\end{bmatrix}  \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} \\lambda v_1 \\\\ \\lambda v_2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} var(x) v_1 + \\Sigma_{xy} v_2 \\\\ \\Sigma_{yx} v_1 + var(y) v_2 \\end{bmatrix}  = \\begin{bmatrix} \\lambda v_1 \\\\ \\lambda v_2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_{xy} v_2 = (\\lambda - var(x)v_1)v_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_{yx}v_1 = (\\lambda - var(y) v_2)v_2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_{xy} v_2 = \\lambda v_1 - var(x)v_1^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_2 = \\frac{\\lambda v_1 - var(x)v_1^2}{\\Sigma_{xy}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_1 = \\frac{\\lambda v_2 - var(y)v_2^2}{\\Sigma_{yx}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_1 = \\frac{\\lambda \\frac{\\lambda v_1 - var(x)v_1^2}{\\Sigma_{xy}} - var(y)( \\frac{\\lambda v_1 - var(x)v_1^2}{\\Sigma_{xy}})^2}{\\Sigma_{yx}}\n",
    "$$\n",
    "\n",
    "After $v_1$ is solved, we just go back and solve for $v_2$. I will put a stop here since the process should be pretty trivial and in reality once we've gain the actual covariance matrix it should be pretty straight forward. \n",
    "\n",
    "The steps of solving for the eigenvalue $\\lambda$ should also be quite simple:\n",
    "\n",
    "$$\n",
    "\\Sigma - \\lambda I = \\begin{bmatrix} var(x) & \\Sigma_{x y} \\\\ \\Sigma_{y x} & var(y) \\end{bmatrix} - \\begin{bmatrix} \\lambda & 0 \\\\ 0  &  \\lambda  \\end{bmatrix} = \\begin{bmatrix} var(x) - \\lambda & \\Sigma_{xy} \\\\ \\Sigma_{yx} & var(y) - \\lambda  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Then we use the determinant to solve for the eigenvalue:\n",
    "\n",
    "$$\n",
    "var(x)var(y) + \\lambda ^2 - \\lambda var(x) - \\lambda var(y) = \\Sigma_{xy} \\Sigma_{yx}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\lambda ^2 - ( var(x) + var(y)) \\lambda - \\Sigma_{xy} \\Sigma_{yx} + var(x)var(y) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda_1 = \\frac{var(x) + var(y) + \\sqrt{( var(x) - var(y))^2 - 4 \\Sigma_{xy} \\Sigma_{yx}}}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda_2 = \\frac{var(x) + var(y) - \\sqrt{( var(x) - var(y))^2 - 4\\Sigma_{xy} \\Sigma_{yx}}}{2}\n",
    "$$\n",
    "\n",
    "All we should do next is to plug in the two eigenvalues, check which one is valid, and then solve for the corresponding eigenvector. Once again, I will pause my solution here since solving for the eigenvectors without any actual value is quite trivial, and as long as the steps I listed above are strictly followed, the calculation should be quite simple. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2. Linear Gaussian Models (20 pts)\n",
    "\n",
    "Consider two multi-dimensional Gaussian random vector variables\n",
    "\n",
    "$$\n",
    "\\newcommand{\\bm}{\\mathbf}\n",
    "\\begin{eqnarray}\n",
    "p(\\bm{x}) &=& \\mathcal{N}(\\bm{x} | \\bm{\\mu_x}, \\bm{\\Sigma}_x) \\\\\n",
    "p(\\bm{z}) &=& \\mathcal{N}(\\bm{z} | \\bm{\\mu_z}, \\bm{\\Sigma}_z)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Now consider a third variable that is the sum of the first two:\n",
    "\n",
    "$$\n",
    "\\bm{y} = \\bm{x} + \\bm{z}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 What is the expression for the distribution $p(\\bm{y})$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, assuming joint normality of $(\\mathbf{x}, \\mathbf{z})$, then we should have, according to the linear properties of multivariate normal random vectors,\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{A} \\mathbf{x} + \\mathbf{B} \\mathbf{z} = (\\mathbf{A} \\space \\space \\mathbf{B})  \\begin{pmatrix}\n",
    "    \\mathbf{x} \\\\  \\mathbf{z}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where both $\\mathbf{A}$ and $\\mathbf{B}$ are identity matrix $\\mathbf{I}$.\n",
    "\n",
    "And thus \n",
    "\n",
    "$$\n",
    "p(\\mathbf{y}) = p(\\mathbf{A} \\mathbf{x} + \\mathbf{B} \\mathbf{z}) =  \\mathcal{N} (\\mathbf{A} \\mathbf{x} + \\mathbf{B} \\mathbf{y} | (\\mathbf{A} \\space \\space \\space  \\mathbf{B})  \\begin{pmatrix} \\mathbf{\\mu_x} \\\\ \\mathbf{\\mu_z} \\end{pmatrix}, (\\mathbf{A} \\space \\space \\space  \\mathbf{B}) \\mathbf{\\Sigma_{x, z} \\begin{pmatrix} \\mathbf{A^T} \\\\ \\mathbf{B^T} \\end{pmatrix}}\n",
    "$$\n",
    "\n",
    "Then we shall plug in $\\mathbf{A} = \\mathbf{B} = \\mathbf{I}$, and the final expression of $p(\\mathbf{y})$ would be:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{y}) = \\mathcal{N} ( \\mathbf{x} + \\mathbf{z} | \\mathbf{\\mu_x} + \\mathbf{\\mu_z}, \\mathbf{ \\Sigma_{xx} } + \\mathbf{\\Sigma_{xz}^T} + \\mathbf{\\Sigma_{xz} } + \\mathbf{ \\Sigma_{zz}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 What is the expression for the condidtional distribution $p(\\bm{y|x})$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Write code that simulates this data and illustrate the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3. Dimensionality Reduction and PCA\n",
    "\n",
    "In this quesiton you will use principal component analysis to reduce the dimensionality of your data and analyze the results.\n",
    "\n",
    "##  3.1 Find a set of high dimensional data.  It should be continuous and have at least 6 dimensions, e.g. stats for sports teams, small sound segments or images patches also work.  Note that if the dimensionality of the data is too large, you might run into computational efficiency problems using standard methods.  Describe the data and illustrate it, if appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for this problem I will be using the breast cancer dataset retrieved from the UCI machine learning database (https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29). Let's first import the data and briefly take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    filepath_or_buffer='http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data', \n",
    "    header = None, \n",
    "    sep = ',')\n",
    "\n",
    "df.columns=['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', \n",
    "            'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', \n",
    "            'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', \n",
    "            'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', \n",
    "            'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', \n",
    "            'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', \n",
    "            'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']\n",
    "\n",
    "## drops the empty line if exist\n",
    "df.dropna(how=\"all\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, there are 32 columns in the data, where the first column is the `id` number, the second column is the label or class `diagnosis` representing the diagnosis of the breast tissues, where `B` = benign, `M` = malignant. The rest of the columns are different continuously valued attributes as described by the column headers above. A more detailed description on the attributes of the dataset can also be found from the source link I previously provided. \n",
    "\n",
    "To make better sense of how the diagnosis results are related to the attributes, we might visualize the attributes with histograms as below. \n",
    "\n",
    "Notice that for the sake of simplicity and efficiency, we will only look at six attributes for our problem 3, which is `radius_worst`, `texture_worst`, `perimeter_worst`, `area_worst`, `smoothness_worst`, and `compactness_worst`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split the dataset into data X and class labels y, the id number will be ignored\n",
    "X = df.iloc[:,22:28].values\n",
    "y = df.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~gentsk77/6.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly\n",
    "import plotly.plotly as py\n",
    "\n",
    "plotly.tools.set_credentials_file(username='gentsk77', api_key='p7wUPaAgt4sO8yQvjk7o')\n",
    "\n",
    "## plotting histograms\n",
    "data = []\n",
    "\n",
    "## muted blue for benign, brick red for malignant\n",
    "colors = {'B': '#1f77b4', 'M': '#d62728'}\n",
    "\n",
    "legend = {0:False, 1:False, 2:False, \n",
    "          3:False, 4:False, 5:True}\n",
    "\n",
    "for col in range(6):\n",
    "    for key in colors:\n",
    "        trace = dict(\n",
    "            type = 'histogram',\n",
    "            x = list(X[y == key, col]),\n",
    "            opacity = 0.75,\n",
    "            xaxis = 'x%s' %(col + 1),\n",
    "            marker = dict(color = colors[key]),\n",
    "            name = key,\n",
    "            showlegend = legend[col]\n",
    "        )\n",
    "        data.append(trace)\n",
    "\n",
    "layout = dict(\n",
    "    barmode = 'overlay',\n",
    "    xaxis1 = dict(domain = [0, 0.14], title = 'worst radius'),\n",
    "    xaxis2 = dict(domain = [0.17, 0.31], title = 'worst texture'),\n",
    "    xaxis3 = dict(domain = [0.34, 0.48], title = 'worst perimeter'),\n",
    "    xaxis4 = dict(domain = [0.51, 0.65], title = 'worst area'),\n",
    "    xaxis5 = dict(domain = [0.68, 0.82], title = 'worst smoothness'),\n",
    "    xaxis6 = dict(domain = [0.85, 1], title = 'worst compactness'),\n",
    "    yaxis = dict(title = 'count'),\n",
    "    title = 'Distribution of breast cancer attributes'\n",
    ")\n",
    "\n",
    "fig = dict(data = data, layout = layout)\n",
    "py.iplot(fig, filename = 'data visualization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Compute the principal components of the data.  Plot a few of the largest eigenvectors and interpret them in terms of how there are modeling the structure of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the eigenvectors of the data, we need to first have the covariance matrix. Let's start with standardizing the data as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_std = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we shall have the covariance matrix as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance matrix \n",
      "[[1.00176056 0.36055442 0.9954574  0.98574698 0.21695572 0.47665775]\n",
      " [0.36055442 1.00176056 0.36574102 0.34645116 0.2258263  0.36146761]\n",
      " [0.9954574  0.36574102 1.00176056 0.97929918 0.23719146 0.53033975]\n",
      " [0.98574698 0.34645116 0.97929918 1.00176056 0.20951355 0.43906793]\n",
      " [0.21695572 0.2258263  0.23719146 0.20951355 1.00176056 0.56918685]\n",
      " [0.47665775 0.36146761 0.53033975 0.43906793 0.56918685 1.00176056]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "mean_vec = np.mean(X_std, axis=0)\n",
    "cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)\n",
    "print('Covariance matrix \\n%s' %cov_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shall perform an eigendecomposition as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvectors \n",
      "[[-4.97942824e-01 -2.72489891e-01 -8.40792994e-02  8.57684507e-02\n",
      "   7.20948420e-01  3.78959132e-01]\n",
      " [-2.73102692e-01  2.28015536e-01  9.29096477e-01  1.00836736e-01\n",
      "  -5.95488854e-03 -2.51370999e-04]\n",
      " [-5.04104514e-01 -2.36650175e-01 -9.49259445e-02  4.93865145e-03\n",
      "  -6.88397894e-01  4.54915759e-01]\n",
      " [-4.90377681e-01 -2.91513263e-01 -8.94458489e-02  1.49878619e-01\n",
      "  -5.63234946e-02 -8.00570578e-01]\n",
      " [-2.28045622e-01  7.07536477e-01 -3.05247522e-01  5.94380987e-01\n",
      "  -1.12294382e-02  2.82209008e-02]\n",
      " [-3.61761561e-01  4.81837130e-01 -1.39724240e-01 -7.78908837e-01\n",
      "   5.48435787e-02 -8.79311954e-02]]\n",
      "\n",
      "Eigenvalues \n",
      "[3.62371526 1.21294634 0.76985699 0.38028693 0.00413964 0.01961823]\n"
     ]
    }
   ],
   "source": [
    "cov_mat = np.cov(X_std.T)\n",
    "\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "print('Eigenvectors \\n%s' %eig_vecs)\n",
    "print('\\nEigenvalues \\n%s' %eig_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The largest eigenvalues are already quite obvious, which are the first three with the corresponding largest eigenvectors. This typically means that the three eigenvectors bear the most information about the dataset, and thus contribute the most to the modeling of the data structure, and thus should be chosen as the principal components while we are trying to reduce the dimension of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Plot, in decreasing order, the cumulative percentage of variance each eigenvector accounts for as a function of the eigenvector number.  These values should be in decreasing order of the eigenvalues. Interpret these results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall first have the eigenvalues and eigenvectors we just computed listed as pairs in the decreasing order so as to compute the cumulative percentage of variance in decreasing order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.623715261672426\n",
      "1.21294633922263\n",
      "0.7698569853011382\n",
      "0.38028692695674704\n",
      "0.019618229300002357\n",
      "0.004139637828750932\n"
     ]
    }
   ],
   "source": [
    "## have all the eigenvalues and eigenvectors listed in pairs\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "## Sort the pairs in decreasing order\n",
    "eig_pairs.sort()\n",
    "eig_pairs.reverse()\n",
    "\n",
    "for p in eig_pairs:\n",
    "    print(p[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we shall plot the cumulative percentage of the variance each eigenvector acccounts for as a function of the eigenvector number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~gentsk77/4.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot = sum(eig_vals)\n",
    "var_exp = [(i / tot) * 100 for i in sorted(eig_vals, reverse = True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "trace = dict(type='scatter',\n",
    "            x=['eigenvector %s' %i for i in range(1,7)], \n",
    "            y=cum_var_exp)\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "layout = dict(title='Cumulative percentage of the variance each eigenvector acccounts for',\n",
    "              yaxis=dict(title='Cumulative percentage of variance'))\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "py.iplot(fig, filename='cumulative percentage of variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, the first two principal components capture 80.5% of the variance. However, it's quite a delimma whether to drop the third principal component or not since it also individually captures almost 13% of the variance. We may make the decision in the next section while making the projection on the selected principal components. Right now we shall just pick the first two eigenvectors as our final principal components. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Plot the original data projected into the space of the two principal eigenvectors (i.e. the eigenvectors with the largest two eigenvalues).  Be sure to either plot relative to the mean, or subtract the mean when you do this.  Interpret your results.  What insights can you draw?  Interpret the dimensions of the two largest principal components.  Which dimensions of the data are correlated?  Or anti-correlated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first reduce our attribute space from 6-dimensional to 2-dimensional subspace since we are only picking two principal eigenvectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.49794282, -0.27248989],\n",
       "       [-0.27310269,  0.22801554],\n",
       "       [-0.50410451, -0.23665018],\n",
       "       [-0.49037768, -0.29151326],\n",
       "       [-0.22804562,  0.70753648],\n",
       "       [-0.36176156,  0.48183713]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_w = np.hstack((eig_pairs[0][1].reshape(6,1), \n",
    "                      eig_pairs[1][1].reshape(6,1)))\n",
    "\n",
    "matrix_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we may perform the transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~gentsk77/10.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = X_std.dot(matrix_w)\n",
    "\n",
    "data = []\n",
    "\n",
    "for name, col in zip(('B', 'M'), colors.values()):\n",
    "    trace = dict(type='scatter',\n",
    "                 x = Y[y == name, 0],\n",
    "                 y = Y[y == name, 1],\n",
    "                 mode = 'markers',\n",
    "                 name = name,\n",
    "                 marker = dict(color=col, size=9, opacity=0.8))\n",
    "    data.append(trace)\n",
    "\n",
    "layout = dict(showlegend= True, \n",
    "              scene = dict(xaxis = dict(title = 'PC1'), \n",
    "                           yaxis=dict(title = 'PC2')))\n",
    "\n",
    "fig = dict(data = data, layout = layout)\n",
    "py.iplot(fig, filename = 'projection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduced subspace separate the two diagnosis pretty well, so it should be sufficient to drop the rest of the dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4. Gaussian Mixture Models\n",
    "\n",
    "## 4.1 Use the EM equations for multivariate Gaussian mixture model to write a program that implements the Gaussian Mixture Model to estimates from an ensemble of data the means, covariance matrices, and class probabilities.  Choose reasonable values for your initial values and a reasonable stopping criterion.  Explain your code and the steps of the algorithm.  Do not assume a diagonal or isotropic covariance matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first derive the equations for our EM algorithm by completing the expectation step and maximization step respectively.\n",
    "\n",
    "### Expectation step\n",
    "\n",
    "By Bayesian, we know that \n",
    "\n",
    "$$\n",
    "p(c_k | x^n, \\theta_{1:K}) = \\frac{p(x^n | c_k, \\theta_{1:K}) p(c_k)}{p(x^n)} = \\frac{p(x^n | c_k, \\theta_{1:K}) p(c_k)}{\\Sigma_k p(x^n | c_k, \\theta_{1:K}) p(c_k)}\n",
    "$$\n",
    "\n",
    "Then, by applying the Gaussian distribution, we are given:\n",
    "\n",
    "$$\n",
    "p(c_k | x^n, \\theta_{1:K}) = \\frac{\\mathcal{N}(x^n | c_k, \\theta_{1:K}) p(c_k)}{\\Sigma_k \\mathcal{N}(x^n | c_k, \\theta_{1:K}) p(c_k)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximization step\n",
    "\n",
    "We will apply the log here to avoid \"something bad\":\n",
    "\n",
    "$$\n",
    "log(p(c_k | x^n, \\theta_{1:K})) = log(\\mathcal{N}(x^n | c_k, \\theta_{1:K}) p(c_k)) - log(\\Sigma_k \\mathcal{N} (x^n | c_k, \\theta_{1:K}) p(c_k)) \n",
    "$$\n",
    "\n",
    "Then to iteratively maximize and update the mean $\\mu_k$, covariance $\\Sigma_k$, and class prior $p_k$, we use the following equations as our update function during each iteration:\n",
    "\n",
    "$$\n",
    "log(\\mu_k') = \\Sigma_n log((p_{n, k}) x^n) - log(p_k)\n",
    "$$\n",
    "\n",
    "$$\n",
    "log(\\Sigma_k') = \\Sigma_n log((p_{n, k}) (x^n - \\mu_k) (x^n - \\mu_k)^T) - log(p_k)\n",
    "$$\n",
    "\n",
    "$$\n",
    "log(p_k') = log(p_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we shall go ahead and implement our algorithm as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute the posterior of each data point \n",
    "def expectation(data, gmm):\n",
    "    num = np.zeros((len(gmm), data.shape[0]))\n",
    "    den = np.zeros((len(gmm), data.shape[0]))\n",
    "    for k in range(len(gmm)):\n",
    "        num[k] = gmm[k][\"prior\"] * multivariate_normal.pdf(data, gmm[k][\"mean\"], gmm[k][\"covariance\"])\n",
    "        for j in range(len(gmm)):\n",
    "            den[k] += gmm[j][\"prior\"] * multivariate_normal.pdf(data, gmm[j][\"mean\"], gmm[j][\"covariance\"])\n",
    "    return np.divide(num, den)\n",
    "\n",
    "## compute new mean, covariance, and class prior for each class\n",
    "def maximization(posterior, data, gmm):\n",
    "    N = np.zeros(len(gmm))\n",
    "    for k in range(N.shape[0]):\n",
    "        N[k] = np.sum(posterior[k])\n",
    "    \n",
    "    mu = np.zeros((len(gmm), len(gmm[0][\"mean\"])))\n",
    "    for k in range(mu.shape[0]):\n",
    "        for n in range(data.shape[0]):\n",
    "            mu[k] += posterior[k, n] * data[n] \n",
    "        gmm[k][\"mean\"] = 1 / N[k] * mu[k]\n",
    "        \n",
    "    prior = np.zeros(len(gmm))\n",
    "    for k in range(prior.shape[0]):\n",
    "        prior[k] = np.divide(N[k], N.sum())\n",
    "        gmm[k][\"prior\"] = prior[k]\n",
    "        \n",
    "    return gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Write code to plot the 3-sigma contours of each Gaussian overlayed on the data (try to find a library function to plot ellipses).  Illustrate with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Define a two-model Gaussian mixture test case, synthesize the data, and verify that your algorithm infers the (approximately) correct values based on training data sampled from the model and plotting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Apply your model to the Old Faithful dataset (supplied with the assignment files).  Run the algorithm for the cases $K=1$, $K=2$, and $K=3$.  For each case, plot the progression of the solutions at the beginning, middle, and final steps in the learning.  For each your plots (you should have 9 total), you should also print out the corresponding values of the mean, covariance, and class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
