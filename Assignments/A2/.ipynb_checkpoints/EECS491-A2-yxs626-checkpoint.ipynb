{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EECS 491 Assignment 2\n",
    "  Yue Shu  \n",
    "  Spring 2019  \n",
    "  Prof. Lewicki  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. Conditional Independence\n",
    "Consider the following Bayes net,\n",
    "![problem 1 image](Images/p1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1\n",
    "**Show that $a$ is independent of $b$ given no other infomration, i.e.\n",
    "$$ a \\perp b\\, |\\, \\varnothing $$**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Proof:* \n",
    "\n",
    "  To prove that $ a \\perp b\\, |\\, \\varnothing $, we need to show that: \n",
    "  $$p(a)\\,p(b) = p(a,b)$$  \n",
    "  According to the DAG above,  \n",
    "  $$ p(a,b) = \\sum_c p(c|a,b) \\, p(a) \\, p(b) $$\n",
    "  $$ p(a,b) = p(a) \\, p(b) \\, \\sum_c p(c|a,b) $$  \n",
    "  And since $ \\sum _c p(c|a,b) = 1$, we may conclude that  \n",
    "  $$ p(a,b) = p(a) \\, p(b)$$  \n",
    "  And therefore, \n",
    "  $$ a \\perp b\\, |\\, \\varnothing $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 \n",
    "**Prove or disprove the following using basic probability (i.e. not using d-separation)\n",
    "$$\n",
    "a \\perp b\\, |\\, e\n",
    "$$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Disproof:* \n",
    "\n",
    "  To disprove that  $ a \\perp b | e $, we need to show that $ p(a|e) \\, p(b|e) \\neq p(a,b|e) $.  \n",
    "  According to Bayes Rule and the DAG above, \n",
    "  $$ p(a,b|e) = \\frac{p(a,b,e)}{p(e)} = \\frac{1}{p(e)} \\sum_{c} p(e|c) \\, p(c|a,b) \\, p(a) \\, p(b) $$  \n",
    "  Since \n",
    "  $$ p(a|e) \\, p(b|e) \\neq \\frac{1}{p(e)} \\sum_{c} p(e|c) \\, p(c|a,b) \\, p(a) \\, p(b) $$  \n",
    "  We may conclude that \n",
    "  $$ p(a|e) \\, p(b|e) \\neq p(a,b|e) $$  \n",
    "  And therefore,  \n",
    "  $$a \\not\\!\\perp\\!\\!\\!\\perp b | e$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. Conditional Independence and Causality\n",
    "\n",
    "Consider the following model\n",
    "![problem 2 image](Images/p2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show that this causal relationship suggested by the arrows does not necessarily hold, because the identical distribution can be represented by a model defined by different conditional distributions.  What conditional independence assumption does this model make?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The causal relationship suggested by the arrows does not necessarily hold because the either the arrow from $a$ to $b$ or the arrow from $a$ to $c$ can be reversed, while the original conditional independence assumption remains the same. For example, the below model represents the alternative possibility: \n",
    "\n",
    "[b] --> [a] --> [c]\n",
    "\n",
    "The conditional independence assumption of the model above is same as the original model, which is $b$ and $c$ are conditionally independent given $a$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. Model Complexity, Free Parameters, and Simplifying Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1\n",
    "\n",
    "**Consider a general probability distribution with $N$ variables $x_1 \\ldots x_N$ each of which can have $K$ values. What is the expression for the joint distribution in terms of conditional probabilities?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint distribution can be expressed as below, where $parents(x_i)$ denotes the cause of $x_i$ with the specific value corresponding to it: \n",
    "\n",
    "$$p(x_1, \\, x_2, \\, ..., \\, x_N) = \\prod_{i = 1}^{N} p(x_i |\\, parents(x_i)) $$\n",
    "$$p(x_1, \\, x_2, \\, ..., \\, x_N) = \\prod_{i = 1}^{N} p(x_i | \\, x_{1}, \\, ..., \\, x_{i - 1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2\n",
    "\n",
    "**What is the total number of free-paramters requried to specify this model?  (Note: the term \"free parameter\" means a parameter that is unconstrained.  For example a Beroulli distribution to describe a coin flip has one free parameter $\\theta$ to describe, say, the probability of heads; the probability of tails must be $1-\\theta$, because the probability is constrained to sum to one.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  There are k possible values for each distinct N variables, so we would need $k^N$ parameters to represent all of the probabilities of the model. However, since the probabilities must sum to 1, the value of the last parameter can be calculated by substracting the sum of the first $k^N - 1$ parameters from 1.  \n",
    "  Therefore, there is a total of $k^N - 1$ free-parameters to specify the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 \n",
    "\n",
    "**Now suppose that the complexity of the model is constrained, so that each variable depends on (at most) $m$ other variables and is conditionally independent of the rest, i.e. each node has $m$ parents and there are $m$ root nodes.  How many parameters are required to define this model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Since now each variable has at most $m$ parents, there are $m$ variables as the parent nodes, and a total of $m$ variables each corresponding to $m$ parent nodes to be specified in order to define the model.  \n",
    "  Therefore, we need (at most) a total of $m  k^{m} - 1$ parameters to define the model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 \n",
    "\n",
    "**Let us make one more simplifying assumption, which is that in addition to depending on only $m$ variables, the conditional probability is described by a noisy-OR function (see Q3).  What is the expression for the number of parameters in this case?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the noisy-OR function in Q4 below, we can conclude that we only need to specify the leak node of variable $x_i$ as well as at most $m$ of its parents in order to define the conditional probability distribution of one single variable. \n",
    "$$ p(x_i | \\textrm{pa}({x_i})) = 1 - (1 - \\mu_{i0}) \\prod_{\\normalsize j \\in \\textrm{pa}(x_i)}(1 - \\mu_{ij})^{x_j} $$\n",
    "Therefore, the expression for the number of parameters $n$ needed to define a complete model in this case is $n \\leq m^2 - 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. Models of Conditional Probability\n",
    "\n",
    "**In Bayesian networks (or directed acyclic graphical models), the joint probability distribution is factored into the product of conditional probability distributions**\n",
    "\n",
    "$$\n",
    "p(x) = \\prod_{i=1}^N p(x_i|\\textrm{pa}(x_i))\n",
    "$$\n",
    "\n",
    "**As we used the previous problem, a simplifying assumption for the conditional probability is noisy-OR model**\n",
    "\n",
    "$$\n",
    "p(x_i | \\textrm{pa}({x_i})) = 1 - (1 - \\mu_{i0}) \\prod_{\\normalsize j \\in \\textrm{pa}(x_i)}(1 - \\mu_{ij})^{x_j}\n",
    "$$\n",
    "\n",
    "**where $j$ is an index over the parents of $x_i$.  Note that the exponent $x_j$ is either 0 or 1 so the term is either 1 or $1-\\mu_{ij}$ depending on the state of the parent $x_j$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1\n",
    "**Show that the noisy-OR function can be interpreted as a \"soft\" (i.e. probabilistic) form of the logical OR function, i.e. the function gives $x_i = 1$ whenever at least one of the parents is 1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  If at least one of the parents is 1, say, $\\mu_{ij} = 1$, then according to the given equation above,   \n",
    "$$ p(x_i | \\textrm{pa}({x_i})) = 1 - (1 - \\mu_{i0}) \\prod_{\\normalsize j \\in \\textrm{pa}(x_i)}(1 - \\mu_{ij})^{x_j} $$\n",
    "$$ p(x_i | \\textrm{pa}({x_i})) = 1 - (1 - \\mu_{i0}) * 0 = 1 - 0 = 1$$\n",
    "  And this is also same for all the rest of the parents of $x_i$ because of the $\\prod_{\\normalsize j \\in \\textrm{pa}(x_i)}(1 - \\mu_{ij})^{x_j}$ component in the equation.  \n",
    "  Therefore, the noisy-OR function can be interpreted as a \"soft\" form of the logical OR function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 \n",
    "**What is the interpretation of $\\mu_{i0}$? Provide a clear explanation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  $\\mu_{i0}$ can be considered as a dummy parameter for $x_i$, which represents the stand alone probability for $x_i$ itself when it does not have any parents, or all of its parents fail. \n",
    "  To make better sense of that, let's first suppose that $x_i$ is independent on all of the other events in the model, which denotes $x_j = 0$, and thus \n",
    "  $$\\prod_{\\normalsize j \\in \\textrm{pa}(x_i)}(1 - \\mu_{ij})^{x_j} = 1$$  \n",
    "  And the final expression of $x_i$ in terms of all of its \"parents\" (which don't necessarily exist) will become \n",
    "  $$ p(x_i | \\textrm{pa}({x_i})) = 1 - (1 - \\mu_{i0})$$\n",
    "  Without the existence of $\\mu_{i0}$, the final outcome of $x_i$ would be 0, which wouldn't make so much sense in this case, where the probability of a completely independent event should just be the probability of itself.  \n",
    "  Furthermore, suppose that $x_i$ has some parents, but all of its parents fail, in which case we would have, same as the previous assumption, \n",
    "  $$\\prod_{\\normalsize j \\in \\textrm{pa}(x_i)}(1 - \\mu_{ij})^{x_j} = 1$$  \n",
    "  And the final expression of $x_i$ in terms of all of its parents will once again become the expression below without the existence of the dummy parameter.  \n",
    "  $$ p(x_i | \\textrm{pa}({x_i})) = 1 - 1 = 0$$\n",
    "  While the actual result of $x_i$ should just be the probability that $x_i$ will happen on its own.  \n",
    "  Therefore, by applying the parameter $\\mu_{i0}$, we could make the result less \"absolute\" and the final result we have for the scenarios above will become $$ p(x_i | \\textrm{pa}({x_i})) = 1 - (1 - \\mu_{i0}) = \\mu_{i0}$$\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another choice for the conditional probability is a sigmoid function**\n",
    "\n",
    "$$\n",
    "p(x_i | \\textrm{pa}({x_i})) = \\sigma\n",
    "\\left(\n",
    "w_{i0} + \\sum_{\\normalsize j \\in \\textrm{pa}(x_i)} w_{ij} x_j\n",
    "\\right)\\,, \\quad \\textrm{where} \\;\n",
    "\\sigma(a) = \\frac{1}{1+e^{-a}}\n",
    "$$\n",
    "\n",
    "**where $\\sigma (a)$ is the logistic sigmoid function.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 \n",
    "**Contrast the noisy-OR function and the sigmoid mathematically.  Is one more general than the other?  Can each compute unique functions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify the mathematical difference between the noisy-OR function and the sigmoid function, let's take a look at the following conditional baysian network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: %3 Pages: 1 -->\r\n",
       "<svg width=\"278pt\" height=\"260pt\"\r\n",
       " viewBox=\"0.00 0.00 278.00 260.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 256)\">\r\n",
       "<title>%3</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-256 274,-256 274,4 -4,4\"/>\r\n",
       "<!-- A -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>A</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-230.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">X1</text>\r\n",
       "</g>\r\n",
       "<!-- E -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>E</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-158.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Z1</text>\r\n",
       "</g>\r\n",
       "<!-- A&#45;&gt;E -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>A&#45;&gt;E</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M27,-215.697C27,-207.983 27,-198.712 27,-190.112\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"30.5001,-190.104 27,-180.104 23.5001,-190.104 30.5001,-190.104\"/>\r\n",
       "</g>\r\n",
       "<!-- B -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>B</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"99\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-230.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">X2</text>\r\n",
       "</g>\r\n",
       "<!-- F -->\r\n",
       "<g id=\"node6\" class=\"node\"><title>F</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"99\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-158.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Z2</text>\r\n",
       "</g>\r\n",
       "<!-- B&#45;&gt;F -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>B&#45;&gt;F</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M99,-215.697C99,-207.983 99,-198.712 99,-190.112\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"102.5,-190.104 99,-180.104 95.5001,-190.104 102.5,-190.104\"/>\r\n",
       "</g>\r\n",
       "<!-- C -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>C</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"171\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"171\" y=\"-230.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">X...</text>\r\n",
       "</g>\r\n",
       "<!-- G -->\r\n",
       "<g id=\"node7\" class=\"node\"><title>G</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"171\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"171\" y=\"-158.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Z...</text>\r\n",
       "</g>\r\n",
       "<!-- C&#45;&gt;G -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>C&#45;&gt;G</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M171,-215.697C171,-207.983 171,-198.712 171,-190.112\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"174.5,-190.104 171,-180.104 167.5,-190.104 174.5,-190.104\"/>\r\n",
       "</g>\r\n",
       "<!-- D -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>D</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"243\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"243\" y=\"-230.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">XN</text>\r\n",
       "</g>\r\n",
       "<!-- H -->\r\n",
       "<g id=\"node8\" class=\"node\"><title>H</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"243\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"243\" y=\"-158.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ZN</text>\r\n",
       "</g>\r\n",
       "<!-- D&#45;&gt;H -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>D&#45;&gt;H</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M243,-215.697C243,-207.983 243,-198.712 243,-190.112\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"246.5,-190.104 243,-180.104 239.5,-190.104 246.5,-190.104\"/>\r\n",
       "</g>\r\n",
       "<!-- M -->\r\n",
       "<g id=\"node9\" class=\"node\"><title>M</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"135\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"135\" y=\"-86.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Z</text>\r\n",
       "</g>\r\n",
       "<!-- E&#45;&gt;M -->\r\n",
       "<g id=\"edge5\" class=\"edge\"><title>E&#45;&gt;M</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M45.8122,-148.807C63.0021,-137.665 88.6184,-121.062 107.993,-108.504\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.916,-111.429 116.403,-103.053 106.108,-105.555 109.916,-111.429\"/>\r\n",
       "</g>\r\n",
       "<!-- F&#45;&gt;M -->\r\n",
       "<g id=\"edge6\" class=\"edge\"><title>F&#45;&gt;M</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M107.35,-144.765C111.712,-136.283 117.147,-125.714 122.041,-116.197\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"125.235,-117.641 126.696,-107.147 119.01,-114.439 125.235,-117.641\"/>\r\n",
       "</g>\r\n",
       "<!-- G&#45;&gt;M -->\r\n",
       "<g id=\"edge7\" class=\"edge\"><title>G&#45;&gt;M</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M162.65,-144.765C158.288,-136.283 152.853,-125.714 147.959,-116.197\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"150.99,-114.439 143.304,-107.147 144.765,-117.641 150.99,-114.439\"/>\r\n",
       "</g>\r\n",
       "<!-- H&#45;&gt;M -->\r\n",
       "<g id=\"edge8\" class=\"edge\"><title>H&#45;&gt;M</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M224.188,-148.807C206.998,-137.665 181.382,-121.062 162.007,-108.504\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"163.892,-105.555 153.597,-103.053 160.084,-111.429 163.892,-105.555\"/>\r\n",
       "</g>\r\n",
       "<!-- J -->\r\n",
       "<g id=\"node10\" class=\"node\"><title>J</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"135\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"135\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Y</text>\r\n",
       "</g>\r\n",
       "<!-- M&#45;&gt;J -->\r\n",
       "<g id=\"edge9\" class=\"edge\"><title>M&#45;&gt;J</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M135,-71.6966C135,-63.9827 135,-54.7125 135,-46.1124\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"138.5,-46.1043 135,-36.1043 131.5,-46.1044 138.5,-46.1043\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x1b01a908>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# A DAG to demonstrate the mathematical difference between noisy-OR and sigmoid function\n",
    "dot = Digraph()\n",
    "dot.node('A', 'X1')\n",
    "dot.node('B', 'X2')\n",
    "dot.node('C', 'X...')\n",
    "dot.node('D', 'XN')\n",
    "dot.node('E', 'Z1') \n",
    "dot.node('F', 'Z2')\n",
    "dot.node('G', 'Z...')\n",
    "dot.node('H', 'ZN')\n",
    "dot.node('M', 'Z')\n",
    "dot.node('J', 'Y')\n",
    "dot.edges(['AE', 'BF', 'CG', 'DH', 'EM', 'FM', 'GM', 'HM', 'MJ'])\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "noisy-OR function:  \n",
    "- substract the product of noise parameters from 1  \n",
    "- the transition from $X_i$ to $Z_i$ represents the simple noisy model in the network  \n",
    "- the transition from $Z_i$ to $Z$ represents the OR-gate logically, and is represented by the $\\prod_{\\normalsize j \\in \\textrm{pa}(x_i)}(1 - \\mu_{ij})^{x_j}$ in the equation from previous parts (ignoring the dummy node)\n",
    "- the transition from $Z$ to $Y$ is simply to substract the value of Z from 1  \n",
    "- the final result maintains the original \"structure\" of the network directly  \n",
    "\n",
    "sigmoid function:\n",
    "- project the generalized linear model on the logistic function  \n",
    "- the transition from $X_i$ to $Z_i$ is through $Z_i = w_iX_i$  \n",
    "- the transition from $Zi$ to $Z$ is to sum all the $Z_i$s up, denoting a generalized linear model  \n",
    "- the transition from $Z$ to $Y$ is through taking $Z$ as a parameter of the sigmoid function  \n",
    "- since the final step is to project $Z$ on the sigmoid function, we do not have the direct manipulation on the result we have  \n",
    "\n",
    "So now that we have a more standardized strucutre of both function, we may come up with some of the conclusions. While both functions pertain the ability to decompose the influence of multiple causes into separate influences, the noise-OR function is more general than the sigmoid function in that the final result comes from a series of product from the original parameter, which directly represents the structure of the network, whereas the sigmoid function does not. \n",
    "\n",
    "For example, in one of the cases I mentioned in part 4.2, suppose all of the parent nodes of a variable $X_i$ fail, then the probability that $X_i$ is true should just be the stand alone probability itself, $\\mu_{i0}$, which is represented by the noise-OR function, whereas the result mapped by the sigmoid function would be $\\frac{1}{1+e^{-{w_{i0}}}}$, which does not represent that feature. \n",
    "\n",
    "I don't quite get what a \"unique function\" is supposed to mean here. But if we're talking about the uniqueness of the function's output given distinct input parameters, neither of the functions shall show the uniqueness, since both function can get duplicate outputs due to their mathematical structures. It is rather simple to play with numerical combinations to get the same result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4\n",
    "**Think of two examples, one for the noisy-OR and one for the sigmoid, that contrast the way these functions model the conditional dependencies.  Explain how each is appropriately modeled by one function but not the other.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. Car Troubles (Barber Exercise 3.6)\n",
    "## 5.1\n",
    "**Calculate the $p(f=\\textsf{empty} | s=\\textsf{no})$, the probability of the fuel tank being empty given that the car does not start.  Do this \"by hand\", i.e in manner similar to the Inference section in Barber 3.1.1.  Use the probabilities given in the exercise.  Show your work.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![problem 5 image](Images/p5.png)\n",
    "![problem 5 image](Images/p5-i.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first come up with the joint distribution as a product of all the conditional distributions according to the given belief network above. \n",
    "$$ p(B,F,G,T,S) = p(G|B,F)p(T|B)p(S|T,F)p(B)p(F) $$\n",
    "\n",
    "$$ p(B,F,G,T,S) = p(G|B,F)p(T|B)p(S|T,F)p(B)p(F) $$\n",
    "Now we can compute the value of $p(f = empty | s = no)$ as below:\n",
    "$$\n",
    "p(F = empty| S = no) = \\frac{p(F = empty, S = no)}{p(S = no)}\n",
    "$$\n",
    "$$\n",
    "p(F = empty| S = no) = \\frac{\\sum_{B,G,T}p(B, F = empty, G, T, S = no)}{\\sum_{B,F,G,T} p(B, F, G, T, S = no)}\n",
    "$$\n",
    "$$\n",
    "p(F = empty| S = no) = \\frac{\\sum_{B,G,T} p(G|B,F=e) p(T|B)p(S = no|T, F=e) p(B) p(F = e)}{\\sum_{B,F,G,T} p(G|B,F)p(T|B)p(S = no|T,F)p(B)p(F)}\n",
    "$$\n",
    "Let's first do some variable elimination to make our life easier:\n",
    "for the numerator, \n",
    "$$\n",
    "numerator = p(F = e)\\sum_T p(S = no|T, F = e) \\sum_B p(T|B) p(B) \\sum_G p(G|B,F=e)\n",
    "$$\n",
    "$$\n",
    "= p(F = e)\\sum_T p(S = no|T, F = e) \\sum_B p(T|B) p(B)\n",
    "$$\n",
    "So the final numerator we get is \n",
    "$$\n",
    "p(F = e)\\sum_T p(S = no|T, F = e) \\sum_B p(T|B) p(B)\n",
    "$$\n",
    "For the denominator, we do the same variable elimination as below: \n",
    "$$\n",
    "denominator = \\sum_F p(F) \\sum_T p(S = no|T,F) \\sum_B p(T|B) p(B) \\sum_G p(G|B,F)\n",
    "$$\n",
    "$$\n",
    "= \\sum_F p(F) \\sum_T p(S = no|T,F) \\sum_B p(T|B) p(B)\n",
    "$$\n",
    "So the final denominator we get is \n",
    "$$\n",
    "\\sum_F p(F) \\sum_T p(S = no|T,F) \\sum_B p(T|B) p(B)\n",
    "$$\n",
    "And the final expression for $p(f = empty | s = no)$ becomes\n",
    "$$\n",
    "\\frac{p(F = e)\\sum_T p(S = no|T, F = e) \\sum_B p(T|B) p(B)}{\\sum_F p(F) \\sum_T p(S = no|T,F) \\sum_B p(T|B) p(B)}\n",
    "$$\n",
    "And now we shall plug in the values we know: \n",
    "$$\n",
    "\\frac{p(F = e)\\sum_T p(S = no|T, F = e) \\sum_B p(T|B) p(B)}{\\sum_F p(F) \\sum_T p(S = no|T,F) \\sum_B p(T|B) p(B)} \n",
    "$$\n",
    "$$\n",
    " = 0.05*(0.99*(0.98*0.02+0.03*0.98) + 0.92*(0.02*0.02+0.97*0.98)) / (0.05*(0.99*(0.98*0.02+0.03*0.98) + 0.92*(0.02*0.02+0.97*0.98)) + 0.95*(1*(0.98*0.02+0.03*0.98) + 0.01*(0.02*0.02+0.97*0.98)))\n",
    "$$\n",
    "$$\n",
    " \\approx 0.4537 \n",
    "$$\n",
    "So the probability of the fuel tank being empty given that the car does not start is approximately $0.4537$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 \n",
    "**Implement this network using a toolbox for probabilistic models (e.g. `pgmpy` or `BayesNets.jl`). Use this to verify that your derivation and calculations are correct for the previous problem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first have the bayesian belief network setup for this model with `pgmpy` as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.factors.discrete import TabularCPD\n",
    "\n",
    "model = BayesianModel([('B', 'G'), ('B', 'T'), ('F', 'G'), ('F', 'S'), ('T', 'S')])\n",
    "\n",
    "priorB = TabularCPD(variable='B', variable_card=2, values=[[0.02, 0.98]]) ## let b = bad denotes 0\n",
    "priorF = TabularCPD(variable='F', variable_card=2, values=[[0.05, 0.95]]) ## let f = empty denotes 0\n",
    "\n",
    "# define p(G|B,F), let G = empty denotes 0\n",
    "cpdG = TabularCPD(variable='G', variable_card=2, \n",
    "    evidence=['B', 'F'], evidence_card=[2, 2],\n",
    "    values=[[0.99, 0.1, 0.97, 0.04], \n",
    "            [0.01, 0.9, 0.03, 0.96]])\n",
    "\n",
    "# define p(T|B), let T = fa denotes 0\n",
    "cpdT = TabularCPD(variable='T', variable_card = 2, \n",
    "    evidence=['B'], evidence_card=[2],\n",
    "    values=[[0.98, 0.03], \n",
    "            [0.02, 0.97]])\n",
    "\n",
    "# define p(S|T,F), let S = fa denotes 0\n",
    "cpdS = TabularCPD(variable='S', variable_card=2, \n",
    "    evidence=['T', 'F'], evidence_card=[2, 2],\n",
    "    values=[[0.99, 1.0, 0.92, 0.01], \n",
    "            [0.01, 0.0, 0.08, 0.99]])\n",
    "\n",
    "model.add_cpds(priorB, priorF, cpdG, cpdT, cpdS)\n",
    "model.check_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we shall play around a little bit with the CPDs to inspect if the network is setup correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| B_0 | 0.02 |\n",
      "+-----+------+\n",
      "| B_1 | 0.98 |\n",
      "+-----+------+\n",
      "+-----+------+\n",
      "| F_0 | 0.05 |\n",
      "+-----+------+\n",
      "| F_1 | 0.95 |\n",
      "+-----+------+\n",
      "+-----+------+------+\n",
      "| B   | B_0  | B_1  |\n",
      "+-----+------+------+\n",
      "| T_0 | 0.98 | 0.03 |\n",
      "+-----+------+------+\n",
      "| T_1 | 0.02 | 0.97 |\n",
      "+-----+------+------+\n",
      "+-----+------+-----+------+------+\n",
      "| B   | B_0  | B_0 | B_1  | B_1  |\n",
      "+-----+------+-----+------+------+\n",
      "| F   | F_0  | F_1 | F_0  | F_1  |\n",
      "+-----+------+-----+------+------+\n",
      "| G_0 | 0.99 | 0.1 | 0.97 | 0.04 |\n",
      "+-----+------+-----+------+------+\n",
      "| G_1 | 0.01 | 0.9 | 0.03 | 0.96 |\n",
      "+-----+------+-----+------+------+\n",
      "+-----+------+-----+------+------+\n",
      "| T   | T_0  | T_0 | T_1  | T_1  |\n",
      "+-----+------+-----+------+------+\n",
      "| F   | F_0  | F_1 | F_0  | F_1  |\n",
      "+-----+------+-----+------+------+\n",
      "| S_0 | 0.99 | 1.0 | 0.92 | 0.01 |\n",
      "+-----+------+-----+------+------+\n",
      "| S_1 | 0.01 | 0.0 | 0.08 | 0.99 |\n",
      "+-----+------+-----+------+------+\n"
     ]
    }
   ],
   "source": [
    "print(model.get_cpds('B'))\n",
    "print(model.get_cpds('F'))\n",
    "print(model.get_cpds('T'))\n",
    "print(model.get_cpds('G'))\n",
    "print(model.get_cpds('S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, it seems that everything turns out pretty well. Now we should get the posterior ready!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "| F   |   phi(F) |\n",
      "|-----+----------|\n",
      "| F_0 |   0.4537 |\n",
      "| F_1 |   0.5463 |\n",
      "+-----+----------+\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "inference = VariableElimination(model)\n",
    "print(inference.query(['F'], evidence={'S' : 0}) ['F'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer looks exactly like the one we got from part 5.1. So we're good to go now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 \n",
    "**Suppose you have loaned this car to a friend. They call call you and announce, \"the car won't start\".  Illustrate your diagnostic and inference process by using the model to show how your beliefs change as you ask questions.  Your friend can only tell you the states of $t$ and $g$ (and you already know $s$).  Use two different scenarios.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the time we picked up the call, we have no prior knowledge about what's going wrong with the car, so we shall take a look at the inference table of all the possible causes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "| F   |   phi(F) |\n",
      "|-----+----------|\n",
      "| F_0 |   0.4537 |\n",
      "| F_1 |   0.5463 |\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "| B   |   phi(B) |\n",
      "|-----+----------|\n",
      "| B_0 |   0.1927 |\n",
      "| B_1 |   0.8073 |\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "| T   |   phi(T) |\n",
      "|-----+----------|\n",
      "| T_0 |   0.4813 |\n",
      "| T_1 |   0.5187 |\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "| G   |   phi(G) |\n",
      "|-----+----------|\n",
      "| G_0 |   0.4732 |\n",
      "| G_1 |   0.5268 |\n",
      "+-----+----------+\n"
     ]
    }
   ],
   "source": [
    "print(inference.query(['F'], evidence={'S' : 0}) ['F'])\n",
    "print(inference.query(['B'], evidence={'S' : 0}) ['B'])\n",
    "print(inference.query(['T'], evidence={'S' : 0}) ['T'])\n",
    "print(inference.query(['G'], evidence={'S' : 0}) ['G'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the beginning, it seems that either the gauge or the turn over of the engine could get wrong given the car won't start, and the major cause of the problem is more likely to be the case where the fuel is empty compared to the possibility that the battery is bad. So we'd need to ask more about the condition of the gauge and the turn over to gain more evidence for our inference. Let's have two different scenarios:  \n",
    "\n",
    "### Scenario One\n",
    "\n",
    "We first asked about the condition of the gauge, and the answer was the gauge was empty. So we shall add the evidence to our inference for scenario one as below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "| F   |   phi(F) |\n",
      "|-----+----------|\n",
      "| F_0 |   0.9306 |\n",
      "| F_1 |   0.0694 |\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "| B   |   phi(B) |\n",
      "|-----+----------|\n",
      "| B_0 |   0.0590 |\n",
      "| B_1 |   0.9410 |\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "| T   |   phi(T) |\n",
      "|-----+----------|\n",
      "| T_0 |   0.1111 |\n",
      "| T_1 |   0.8889 |\n",
      "+-----+----------+\n"
     ]
    }
   ],
   "source": [
    "inference1 = VariableElimination(model)\n",
    "print(inference1.query(['F'], evidence={'S' : 0, 'G' : 0}) ['F'])\n",
    "print(inference1.query(['B'], evidence={'S' : 0, 'G' : 0}) ['B'])\n",
    "print(inference1.query(['T'], evidence={'S' : 0, 'G' : 0}) ['T'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  We shall see how the possibility of the fuel being empty dramatically increases given that the car won't start and the gauge is empty. Also, our belief on the condition of the car's engine changes as well, as right now we believe that the car engine is very unlikely to turn over compared to our original belief.   \n",
    "  Now we can gain more information about the engine's condition to confirm our belief.  \n",
    "  We asked our dear friend if the engine would turn over, and he seemed to loose his patience: \"No!\"  \n",
    "  Good guess on the engine. Let's add that evidence to our inference and see how that changes our belief on the condition of the car's battery and fuel tank:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "| F   |   phi(F) |\n",
      "|-----+----------|\n",
      "| F_0 |   0.4433 |\n",
      "| F_1 |   0.5567 |\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "| B   |   phi(B) |\n",
      "|-----+----------|\n",
      "| B_0 |   0.5274 |\n",
      "| B_1 |   0.4726 |\n",
      "+-----+----------+\n"
     ]
    }
   ],
   "source": [
    "print(inference1.query(['F'], evidence={'S' : 0, 'G' : 0, 'T' : 0}) ['F'])\n",
    "print(inference1.query(['B'], evidence={'S' : 0, 'G' : 0, 'T' : 0}) ['B'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see a dramatic increase in the possibility that the battery was bad and a dramatic decrease in the possibility that the fuel tank was empty given the car won't start, the gauge was empty, and the engine wouldn't turn over. But it's most likely a half-and-half chance, so we still couldn't know what on earth is wrong with the car -- or could it be both the battery and the fuel tank were having some problems? The car mechanics should figure it out, while as a CS student learning probability, I shall gladly take the numbers. Chill isn't it? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario Two\n",
    "\n",
    "  This time, when we asked our friend about the condition of the gauge, he seemed pretty confused since apparently the gauge wasn't empty yet. So let's add that evidence to our inference query: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "| F   |   phi(F) |\n",
      "|-----+----------|\n",
      "| F_0 |   0.0255 |\n",
      "| F_1 |   0.9745 |\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "| B   |   phi(B) |\n",
      "|-----+----------|\n",
      "| B_0 |   0.3128 |\n",
      "| B_1 |   0.6872 |\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "| T   |   phi(T) |\n",
      "|-----+----------|\n",
      "| T_0 |   0.8137 |\n",
      "| T_1 |   0.1863 |\n",
      "+-----+----------+\n"
     ]
    }
   ],
   "source": [
    "inference2 = VariableElimination(model)\n",
    "print(inference2.query(['F'], evidence={'S' : 0, 'G' : 1}) ['F'])\n",
    "print(inference2.query(['B'], evidence={'S' : 0, 'G' : 1}) ['B'])\n",
    "print(inference2.query(['T'], evidence={'S' : 0, 'G' : 1}) ['T'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  It seems that we're getting closer to an answer, since the probability that the fuel tank is empty given the car won't start and the gauge isn't empty is prettly low, while there is a certain increase in the possibility that the car has a bad battery given the evidence above. Furthermore, it seems very likely to us that the engine won't turn over given the information we had.  \n",
    "  So to make sure that there could be actually something wrong with the battery, we asked our friend if the engine could turn over, and he seemed pretty certain that it couldn't.  \n",
    "  Given the information above, again let's update our evidence for the inference: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "| F   |   phi(F) |\n",
      "|-----+----------|\n",
      "| F_0 |   0.0012 |\n",
      "| F_1 |   0.9988 |\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "| B   |   phi(B) |\n",
      "|-----+----------|\n",
      "| B_0 |   0.3844 |\n",
      "| B_1 |   0.6156 |\n",
      "+-----+----------+\n"
     ]
    }
   ],
   "source": [
    "print(inference2.query(['F'], evidence={'S' : 0, 'G' : 1, 'T' : 0}) ['F'])\n",
    "print(inference2.query(['B'], evidence={'S' : 0, 'G' : 1, 'T' : 0}) ['B'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  This time, according to the table above, it seems that given the car won't start, the gauge isn't empty, and the engine won't turn over, the possibility that the fuel tank was empty is actually very low and we can almost neglect that chance.  \n",
    "  Still, the chance that the battery is bad isn't that high, but we can see a certain increase in the number. And a roughly forty percent chance definitely worth the try to either recharge or replace the battery to see how it turns out.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
